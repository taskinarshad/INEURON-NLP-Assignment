{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9caf71",
   "metadata": {},
   "source": [
    "1. What are Corpora?\n",
    "\n",
    "ANS:- Corpora (singular: corpus) are large and structured collections of text or speech data that are used for linguistic research, language analysis, and various natural language processing (NLP) tasks. These collections are typically created to study the patterns, characteristics, and properties of a language or languages. Corpora can take different forms:\n",
    "\n",
    "     1.Text Corpora: These consist of written text and can include a wide range of materials, such as books, articles, websites, newspapers, and more. Text corpora are often categorized by genre, topic, language, or time period.\n",
    "\n",
    "     2.Speech Corpora: These contain recordings of spoken language, including transcriptions of audio data. Speech corpora are essential for studying phonetics, phonology, and other aspects of spoken language.\n",
    "\n",
    "     3.Multimodal Corpora: These include a combination of text, speech, images, and sometimes even video. They are used to analyze and understand how language and other forms of communication are integrated.\n",
    "\n",
    "Corpora are essential resources for linguists, computational linguists, and NLP researchers because they provide the raw data needed to investigate language structure, syntax, semantics, and pragmatics. They are also used to train and evaluate machine learning models, such as those used in machine translation, sentiment analysis, and speech recognition.\n",
    "\n",
    "Corpora can be created manually by linguists who annotate and transcribe texts or speech, or they can be harvested from existing sources, such as websites or books. Some widely known corpora include the Penn Treebank for English, the Brown Corpus for American English, and the Corpus of Contemporary American English (COCA), among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430734f",
   "metadata": {},
   "source": [
    "2. What are Tokens?\n",
    "\n",
    "ANS:-Tokens are the basic units or elements into which a text or a speech segment is divided for the purpose of linguistic analysis and natural language processing (NLP). In the context of language processing, a token is typically a word, a symbol, or a piece of text that is separated from other tokens by spaces, punctuation, or other delimiters. Tokens are fundamental for various language-related tasks, including parsing, text analysis, and machine learning. Here are a few key points about tokens:\n",
    "\n",
    "    1.Word Tokens: In most cases, tokens represent individual words in a text. For example, in the sentence \"The quick brown fox jumps,\" there are five word tokens: \"The,\" \"quick,\" \"brown,\" \"fox,\" and \"jumps.\"\n",
    "\n",
    "    2.Punctuation Tokens: Punctuation marks, such as periods, commas, exclamation points, and question marks, are often treated as separate tokens. For instance, in the sentence \"Hello, how are you?\" there are five tokens: \"Hello,\" \",\", \"how,\" \"are,\" and \"you.\"\n",
    "\n",
    "    3.Symbol Tokens: Tokens can also represent symbols, numbers, or other non-word elements. For example, in the expression \"3.14 + 2 = 5.14,\" the tokens are \"3.14,\" \"+,\" \"2,\" \"=\", and \"5.14.\"\n",
    "\n",
    "    4.Tokenization: The process of breaking down a text or a speech segment into individual tokens is called tokenization. Tokenization is a crucial step in NLP and is often one of the first steps in text processing pipelines.\n",
    "\n",
    "    5.Token Count: The number of tokens in a text is referred to as the token count. It is a useful metric for various NLP tasks, such as calculating the length of a document, measuring vocabulary size, or assessing the complexity of a text.\n",
    "\n",
    "    6.Text Analysis: Tokens serve as the foundation for various text analysis tasks, including part-of-speech tagging, named entity recognition, sentiment analysis, and language modeling. These tasks involve analyzing the properties and relationships of individual tokens within a text.\n",
    "\n",
    "In summary, tokens are the atomic units of text or speech data that are used for linguistic analysis and processing. They provide the basis for understanding and working with language in both human and machine contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058be70",
   "metadata": {},
   "source": [
    "3. What are Unigrams, Bigrams, Trigrams?\n",
    "\n",
    "ANS:- Unigrams, bigrams, and trigrams are different types of n-grams, which are consecutive sequences of n items (typically words or tokens) in a text or speech. These n-grams are used in various natural language processing (NLP) tasks, such as text analysis, language modeling, and machine learning. Let's define each of these terms:\n",
    "\n",
    "   1.Unigrams (1-grams):\n",
    "\n",
    "Unigrams are individual words or tokens in a text or speech corpus.\n",
    "They represent the simplest form of n-grams and do not involve grouping words together.\n",
    "For example, in the sentence \"I love natural language processing,\" the unigrams are: \"I,\" \"love,\" \"natural,\" \"language,\" \"processing.\"\n",
    "Bigrams (2-grams):\n",
    "\n",
    "   2.Bigrams consist of pairs of consecutive words or tokens in a text.\n",
    "They are created by moving a sliding window of size 2 across the text, capturing every pair of adjacent words.\n",
    "For example, in the sentence \"I love natural language processing,\" the bigrams are: \"I love,\" \"love natural,\" \"natural language,\" \"language processing.\"\n",
    "Trigrams (3-grams):\n",
    "\n",
    "   3.Trigrams consist of groups of three consecutive words or tokens in a text.\n",
    "Similar to bigrams, they are created by using a sliding window, but with a window size of 3.\n",
    "For example, in the sentence \"I love natural language processing,\" the trigrams are: \"I love natural,\" \"love natural language,\" \"natural language processing.\"\n",
    "N-grams are useful in NLP and text analysis for various purposes:\n",
    "\n",
    "Statistical Language Modeling: N-grams, especially bigrams and trigrams, are used in language modeling to estimate the likelihood of a sequence of words occurring in a given language. This is the basis for tasks like predicting the next word in a sentence.\n",
    "\n",
    "Text Classification: N-grams can be used as features for text classification tasks. By considering the presence or frequency of specific n-grams, machine learning models can make predictions about the category or sentiment of a text.\n",
    "\n",
    "Information Retrieval: N-grams are used in information retrieval systems to improve search accuracy. They can help identify relevant documents based on matching n-grams in search queries and document content.\n",
    "\n",
    "Text Generation: N-grams can be used to generate text, especially in the case of n-gram-based language models. These models can produce coherent text by selecting the next word based on the preceding n-grams.\n",
    "\n",
    "The choice of n (1, 2, 3, etc.) depends on the specific NLP task and the level of context required for analysis or modeling. Larger n-grams capture more contextual information but may result in sparser data, while smaller n-grams provide less context but may yield denser data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50294b44",
   "metadata": {},
   "source": [
    "4. How to generate n-grams from text?\n",
    "\n",
    "\n",
    "ANS:-Generating n-grams from text involves breaking down a text or a sequence of words into consecutive n-word (or token) combinations. You can create n-grams from a given text using programming libraries or by implementing the logic yourself. Here's a general approach to generate n-grams:\n",
    "\n",
    "   1.Tokenization: Before generating n-grams, you need to tokenize the input text into individual words or tokens. This involves splitting the text into its constituent units, typically using spaces as separators. Many programming languages and NLP libraries provide tokenization functions or libraries to help with this step.\n",
    "\n",
    "   2.Create N-Grams:\n",
    "\n",
    "Initialize an empty list to store the n-grams.\n",
    "Use a sliding window approach to iterate through the tokens in the text. Start with the first n tokens (for n-grams of size n).\n",
    "At each step, extract the current window of n tokens and add it to the list of n-grams.\n",
    "Move the window one token to the right and repeat until you reach the end of the text.\n",
    "\n",
    "Here's some example Python code to generate n-grams from a given text using a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e4f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    # Tokenize the text (assuming space-separated tokens)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Initialize an empty list to store n-grams\n",
    "    ngrams = []\n",
    "    \n",
    "    # Iterate through the tokens to create n-grams\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = ' '.join(tokens[i:i + n])\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67102da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['I love', 'love natural', 'natural language', 'language processing']\n",
      "Trigrams: ['I love natural', 'love natural language', 'natural language processing']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love natural language processing\"\n",
    "bigrams = generate_ngrams(text, 2)\n",
    "trigrams = generate_ngrams(text, 3)\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184ff9d",
   "metadata": {},
   "source": [
    "5. Explain Lemmatization?\n",
    "\n",
    "ANS:-Lemmatization is a natural language processing (NLP) technique that involves reducing words to their base or canonical form, known as the lemma, while preserving their meaning. The purpose of lemmatization is to standardize words and reduce inflected or derived forms to a common base, making it easier to analyze and compare words in text processing tasks. Lemmatization is particularly useful for tasks like text classification, information retrieval, and language modeling. Here are some key points about lemmatization:\n",
    "\n",
    "    1.Lemma: The lemma of a word is its base or dictionary form, which represents the word's core meaning. For example, the lemma of the word \"running\" is \"run,\" and the lemma of \"better\" is \"good.\"\n",
    "\n",
    "    2.Inflected Forms: In natural language, words can appear in different forms based on tense, number, gender, and other grammatical features. For instance, the word \"walk\" can appear as \"walks,\" \"walked,\" or \"walking.\" Lemmatization reduces all these forms to the common lemma \"walk.\"\n",
    "\n",
    "    3.Morphological Analysis: Lemmatization often involves morphological analysis, which considers the structure of words and their affixes (prefixes and suffixes) to determine the lemma. Morphological rules and dictionaries are used to identify lemmas accurately.\n",
    "\n",
    "    4.Preservation of Meaning: Unlike stemming, another text normalization technique, lemmatization aims to preserve the semantic meaning of words. Stemming may produce stems that are not actual words, whereas lemmatization ensures that the resulting lemma is a valid word in the language.\n",
    "\n",
    "    5.Part-of-Speech Consideration: Lemmatization can take into account the part of speech of a word. For example, the lemma of \"better\" can be different depending on whether it is used as an adjective or an adverb. Contextual information is often used to disambiguate.\n",
    "\n",
    "    6.Lemmatization Tools: Many NLP libraries and tools provide lemmatization capabilities. These tools typically include linguistic databases or models that map words to their lemmas. Popular NLP libraries like NLTK (Natural Language Toolkit) in Python and spaCy offer lemmatization functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfba2b6",
   "metadata": {},
   "source": [
    "6. Explain Stemming ?\n",
    "\n",
    "\n",
    "ANS:-Stemming is a text normalization technique used in natural language processing (NLP) and information retrieval to reduce words to their root or base form, called the \"stem.\" The goal of stemming is to remove affixes (prefixes and suffixes) from words in order to group together words with similar meanings. Stemming can help in tasks like information retrieval, search engines, and text analysis, although it may not always produce valid words or preserve the original meaning of a word. Here are some key points about stemming:\n",
    "\n",
    "     1.Stem: The stem of a word is the core part that remains after removing affixes. For example, the stem of \"jumping\" is \"jump,\" and the stem of \"running\" is \"run.\"\n",
    "\n",
    "     2.Affixes: Affixes are added to words to create different grammatical forms or derived words. Common affixes include prefixes (e.g., \"un-\" in \"undo\") and suffixes (e.g., \"-ing\" in \"jumping\" or \"-ed\" in \"walked\").\n",
    "\n",
    "     3.Heuristic Approach: Stemming algorithms use heuristic rules to identify and remove affixes from words. These rules are based on patterns in the language, and stemming may not always be perfect because it can produce stems that are not valid words.\n",
    "\n",
    "     4.Overstemming and Understemming: Stemming can result in overstemming, where multiple words are stemmed to the same form even if they have different meanings (e.g., \"jumped\" and \"jumper\" both stemming to \"jump\"). Conversely, stemming can also lead to understemming, where words with similar meanings are not stemmed to the same form.\n",
    "\n",
    "     5.Porter Stemming Algorithm: The Porter stemming algorithm is one of the most widely used stemming algorithms. It applies a series of rules to reduce words to their stems. For example, it applies rules like removing common suffixes (\"-ed,\" \"-ing\") and converting irregular plurals to their singular forms.\n",
    "\n",
    "     6.Snowball Stemming: The Snowball stemming algorithm, also known as the Porter2 stemming algorithm, is an improved version of the Porter algorithm and is more aggressive in its stemming rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8a774",
   "metadata": {},
   "source": [
    "Here's an example of stemming in Python using the NLTK library, which includes the Porter stemming algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32d5f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jump', 'jump', 'jump', 'friendli', 'friendship']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words to be stemmed\n",
    "words = [\"jumps\", \"jumped\", \"jumping\", \"friendly\", \"friendship\"]\n",
    "\n",
    "# Stem the words\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efe42b",
   "metadata": {},
   "source": [
    "7. Explain Part-of-speech (POS) tagging ?\n",
    "\n",
    "ANS:- \n",
    "Part-of-speech (POS) tagging, also known as grammatical tagging or word-category disambiguation, is a fundamental natural language processing (NLP) task that involves assigning a grammatical category or part-of-speech label to each word in a sentence or text. The goal of POS tagging is to determine the syntactic function of each word in the context of a sentence. POS tags provide information about how words are used in a sentence, which is essential for various NLP tasks, such as parsing, information retrieval, and machine translation. Here are some key points about POS tagging:\n",
    "\n",
    "    1.Part-of-Speech Categories: In English and many other languages, words can be categorized into various grammatical classes or parts of speech, including nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, and more. Each part of speech serves a specific grammatical function in a sentence.\n",
    "\n",
    "    2.POS Tags: POS tags are short labels or codes that represent the grammatical category of a word. These tags are typically standardized according to a specific tag set or linguistic framework. For example, in the Penn Treebank POS tag set, nouns are tagged as \"NN,\" verbs as \"VB,\" adjectives as \"JJ,\" and so on.\n",
    "\n",
    "    3.Ambiguity: Many words in natural language can have multiple meanings and can function as different parts of speech depending on the context. POS tagging helps disambiguate such words by determining their appropriate grammatical category based on the surrounding words.\n",
    "\n",
    "    4.Context Sensitivity: POS tagging takes into account the immediate context of each word in a sentence. The same word may be tagged differently in different sentences based on its role in those sentences.\n",
    " \n",
    "    5.Applications: POS tagging is a crucial preprocessing step for various NLP tasks, including:\n",
    "\n",
    "Parsing: It aids in syntactic parsing, where the grammatical structure of sentences is analyzed.\n",
    "    \n",
    "Information Retrieval: It helps in improving search accuracy by considering the part of speech of query terms and document words.\n",
    "    \n",
    "Named Entity Recognition: It assists in identifying named entities (e.g., person names, locations) by recognizing patterns of POS tags.\n",
    "    \n",
    "Machine Translation: It can inform translation models about the grammatical function of words in the source language, helping produce more accurate translations.\n",
    "    \n",
    "   6.POS Tagging Algorithms: POS tagging can be performed using rule-based methods, statistical methods, or machine learning techniques. Statistical methods often involve training a model on labeled corpora to predict POS tags for words based on their context.\n",
    "\n",
    "Here's a simplified example of POS tagging for the sentence \"She is reading a book\" using the Penn Treebank POS tag set:\n",
    "\n",
    "\"She\" is tagged as a pronoun (PRP).\n",
    "\"is\" is tagged as a verb (VBZ).\n",
    "\"reading\" is tagged as a verb (VBG).\n",
    "\"a\" is tagged as a determiner (DT).\n",
    "\"book\" is tagged as a noun (NN).\n",
    "POS tagging plays a critical role in understanding the grammatical structure of text and is a foundational component of many NLP systems and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7abb29",
   "metadata": {},
   "source": [
    "8. Explain Chunking or shallow parsing ?\n",
    "\n",
    "ANS:-\n",
    "Chunking, also known as shallow parsing, is a natural language processing (NLP) technique that involves identifying and grouping together contiguous words or tokens in a sentence that form meaningful syntactic units, such as noun phrases (NP) or verb phrases (VP). Chunking is less detailed than full syntactic parsing (parsing trees), as it does not create a complete syntactic structure of a sentence but focuses on identifying key phrases or chunks. Chunking is often used as an intermediate step in NLP tasks and can be particularly useful for extracting structured information from text data. Here are some key points about chunking:\n",
    "\n",
    "    1.Chunk Types: Chunking typically involves identifying and labeling chunks of specific types, such as noun phrases (NP), verb phrases (VP), prepositional phrases (PP), and more. These chunks are often defined based on linguistic patterns and syntactic rules.\n",
    "\n",
    "    2.Regular Expressions or Rules: Chunking is often implemented using regular expressions or rule-based approaches. Linguistic rules are applied to identify the boundaries and content of chunks. For example, a rule might specify that an NP chunk starts with a determiner (DT) followed by one or more adjectives (JJ) and ends with a noun (NN).\n",
    "\n",
    "    3.Example Chunking:\n",
    "\n",
    "Given the sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "A chunker might identify the following noun phrases (NP):\n",
    "\"The quick brown fox\"\n",
    "\"the lazy dog\"\n",
    "And verb phrases (VP):\n",
    "\"jumps over\"\n",
    "     4 .Applications: Chunking is used in various NLP applications, including:\n",
    "\n",
    "Information Extraction: It helps extract structured information from unstructured text data by identifying relevant chunks (e.g., extracting product names or dates).\n",
    "Named Entity Recognition (NER): NER systems often rely on chunking to identify and label named entities like person names, locations, and organizations.\n",
    "Question Answering: Chunking can assist in identifying the parts of a question that correspond to specific answer categories (e.g., the subject or object of a question).\n",
    "Text Summarization: It can be used to identify important phrases or chunks for generating summaries of longer texts.\n",
    "    5.Chunking Tools: Many NLP libraries, such as NLTK (Natural Language Toolkit) in Python and spaCy, provide pre-trained chunkers and tools for performing chunking tasks. These libraries allow you to extract structured information from text easily.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c94084",
   "metadata": {},
   "source": [
    "9. Explain Noun Phrase (NP) chunking?\n",
    "\n",
    "ANS:-\n",
    "Noun Phrase (NP) chunking is a specific type of shallow parsing or chunking that focuses on identifying and extracting noun phrases from sentences or text. A noun phrase is a syntactic structure that consists of a noun (a person, place, thing, or idea) along with its associated modifiers, such as adjectives, determiners, and prepositional phrases. NP chunking is a common natural language processing (NLP) task used to extract and analyze these noun phrases in text data. Here are the key points about NP chunking:\n",
    "\n",
    "    1.Definition of Noun Phrase (NP): A noun phrase typically includes:\n",
    "\n",
    "A noun (NN): The core element of the phrase, representing the main subject or object.\n",
    "Optional adjectives (JJ): Words that describe or modify the noun.\n",
    "Optional determiners (DT): Words that specify whether the noun is definite (e.g., \"the\") or indefinite (e.g., \"a\" or \"an\").\n",
    "Optional prepositional phrases (PP): Phrases that provide additional information about the noun and usually begin with a preposition (e.g., \"of the world\").\n",
    "    2.NP Chunking Process: NP chunking involves identifying and grouping words that constitute a noun phrase in a sentence. This is typically done using linguistic rules, regular expressions, or machine learning techniques. Some common rules used in NP chunking include:\n",
    "\n",
    "Detecting sequences of determiners, adjectives, and nouns.\n",
    "Recognizing prepositional phrases that modify nouns.\n",
    "Applying part-of-speech tagging to identify noun-related words.\n",
    "    3.Applications of NP Chunking:\n",
    "\n",
    "Information Extraction: NP chunking is useful for extracting structured information from unstructured text, such as extracting names of people, places, or objects.\n",
    "Named Entity Recognition (NER): Many named entities, like person names and organization names, can be found within noun phrases. NP chunking can be an initial step in NER systems.\n",
    "Question Answering: NP chunking helps identify the key noun phrases in a question that may serve as potential answers.\n",
    "Text Summarization: When summarizing text, NP chunking can help identify the most important noun phrases to include in a summary.\n",
    "    4.Example of NP Chunking:\n",
    "\n",
    "Sentence: \"The big brown dog chased the squirrel in the park.\"\n",
    "NP Chunks:\n",
    "\"The big brown dog\"\n",
    "\"the squirrel\"\n",
    "\"the park\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d68da9",
   "metadata": {},
   "source": [
    "10. Explain Named Entity Recognition ?\n",
    "\n",
    "ANS:-\n",
    "Named Entity Recognition (NER) is a natural language processing (NLP) technique that involves identifying and classifying named entities in text data. Named entities are specific words or phrases that refer to unique individuals, locations, organizations, dates, numerical values, and other categories of objects or concepts. NER plays a crucial role in various NLP applications and information extraction tasks by extracting structured information from unstructured text. Here are the key points about Named Entity Recognition:\n",
    "\n",
    "    1.Named Entities: Named entities can be categorized into several types, including but not limited to:\n",
    "\n",
    "Person Names: Individual names of people, such as \"John Smith\" or \"Jane Doe.\"\n",
    "Location Names: Names of places, such as \"New York City\" or \"Mount Everest.\"\n",
    "Organization Names: Names of companies, institutions, or organizations, such as \"Google\" or \"Harvard University.\"\n",
    "Date Expressions: References to specific dates, such as \"January 1, 2022\" or \"the 20th century.\"\n",
    "Numeric Values: Numerical expressions, such as percentages, monetary amounts, or measurements, like \"$100\" or \"10 miles.\"\n",
    "    2.NER Process: Named Entity Recognition typically involves the following steps:\n",
    "\n",
    "Tokenization: The input text is divided into individual words or tokens.\n",
    "Part-of-Speech Tagging (Optional): POS tagging may be applied to identify the grammatical category of each word, which can help in NER.\n",
    "NER Tagging: Each token is tagged with a label that indicates its named entity type (e.g., PERSON, LOCATION, ORGANIZATION, DATE).\n",
    "Classification: The NER model uses linguistic patterns, contextual information, and sometimes machine learning algorithms to classify tokens into named entity categories.\n",
    "    3.NER Applications:\n",
    "\n",
    "Information Extraction: NER is used to extract structured information from unstructured text, enabling the retrieval of specific details like names of people, organizations, and dates from documents.\n",
    "Question Answering: NER helps identify and extract entities that can serve as answers to questions in question-answering systems.\n",
    "Document Summarization: Named entities are often key elements in summarizing documents, as they provide context and relevance.\n",
    "Machine Translation: In machine translation, NER can help identify and translate named entities accurately.\n",
    "Search Engines: NER assists search engines in understanding user queries and returning relevant results based on entity recognition.\n",
    "    4.NER Tools and Models: NER can be performed using rule-based approaches, statistical models, or deep learning models. Common NER tools and libraries, like spaCy and NLTK in Python, offer pre-trained NER models for various languages and entity types.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00865e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
